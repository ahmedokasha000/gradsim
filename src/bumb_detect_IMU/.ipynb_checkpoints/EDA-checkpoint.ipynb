{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.signal as sg\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error,precision_recall_curve, precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold as StK\n",
    "%matplotlib qt\n",
    "pd.set_option(\"display.max_rows\",50)\n",
    "pd.set_option(\"display.max_columns\",50)\n",
    "pd.set_option(\"max_colwidth\",20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3ddb0b3b6f7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mfiltered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbutter_bandpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiltered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def butter_bandpass(data, rang, fs):\n",
    "    \"\"\"\n",
    "    bandpass the data withn certain frequency range for all sensors\n",
    "    Args:\n",
    "        data: a list of sensors measurements\n",
    "        range: tuple contain the min, max frequency\n",
    "        fs: sample frequence for the signal\n",
    "    Returns:\n",
    "        return a list arrays for all the filtered signals in the time domain\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    zeros, poles = sg.butter(3, rang, btype=\"bandpass\", fs=fs)\n",
    "    # filter each sensor measurement\n",
    "    for key in data:\n",
    "        results.append(sg.filtfilt(zeros, poles, key))\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# in your prediction file\n",
    "def load_model(model_path):\n",
    "    \"\"\"\n",
    "    Loads a saved sklearn regression model or train new model\n",
    "    Args:\n",
    "    Returns:\n",
    "        return the saved model\n",
    "    \"\"\"\n",
    "    path_n = model_path\n",
    "    if os.path.isfile(path_n):\n",
    "        fil = open(path_n, \"rb\")\n",
    "        prediction_model = pickle.load(fil)\n",
    "        print(\"model found\")\n",
    "    else:\n",
    "        prediction_model = model_train()\n",
    "        print(\"new model is created\")\n",
    "    return prediction_model\n",
    "\n",
    "def load_all_dataset(dir, fs, window_size=5, window_overlab=2):\n",
    "\n",
    "    \"\"\"\n",
    "    Load data, labels from all files, windowing on each 6 seconds data with\n",
    "    4 sec overlab each sample data is a window of 6 seconds and shifted 2 s\n",
    "    from the previoues one\n",
    "    Args:\n",
    "        data_fls: all the files for the dataset sensor measurements\n",
    "        ref_fls: all the files for the dataset sensor labels\n",
    "        fs: sample frequency\n",
    "    Returns: list of array for all samples in the dataset, each sample data\n",
    "     has all the sensors measurements in this sample in the time domain\n",
    "    \"\"\"\n",
    "\n",
    "    head=[\"acc_x\",\"acc_y\",\"acc_z\",\"ang_vel_x\",\"ang_vel_y\",\"ang_vel_z\",\"angle_x\",\"angle_y\",\"angle_z\",\"bump_class\"]\n",
    "    df=pd.read_csv(dir,sep=\",\")\n",
    "    df.columns=head\n",
    "\n",
    "    label_peaks, _ = sg.find_peaks(\n",
    "                df[\"bump_class\"].values, distance=200\n",
    "            )\n",
    "\n",
    "    for peak in label_peaks:\n",
    "        df[\"bump_class\"].iloc[peak-250:peak+250]=1\n",
    "        \n",
    "    X = []\n",
    "    Y = []\n",
    "    window_len = window_size * fs\n",
    "    window_shift_len = (window_size - window_overlab) * fs\n",
    "  \n",
    "    # Convert dataframe to samples each one is shifted 2s\n",
    "    number_samples = int(len(df) / window_shift_len)\n",
    "    for window_ind in range(number_samples):\n",
    "        sample_start = int(window_ind * window_shift_len)\n",
    "        sample_end = int(sample_start + window_len)\n",
    "        acc_x = df[\"acc_x\"][sample_start:sample_end].values\n",
    "        acc_y = df[\"acc_y\"][sample_start:sample_end].values\n",
    "        acc_z = df[\"acc_z\"][sample_start:sample_end].values\n",
    "        ang_vel_x=df[\"ang_vel_x\"][sample_start:sample_end].values\n",
    "        ang_vel_y=df[\"ang_vel_y\"][sample_start:sample_end].values\n",
    "        ang_vel_z=df[\"ang_vel_z\"][sample_start:sample_end].values\n",
    "        ang_x=df[\"angle_x\"][sample_start:sample_end].values\n",
    "        ang_y=df[\"angle_y\"][sample_start:sample_end].values\n",
    "        ang_z=df[\"angle_z\"][sample_start:sample_end].values\n",
    "        labels = df[\"bump_class\"][sample_start:sample_end].values\n",
    "        # [acc_x, acc_y, acc_z, ang_vel_x,ang_vel_y,ang_vel_z,ang_x,ang_y,ang_z]\n",
    "        X.append(np.array([acc_x, acc_y, acc_z,ang_vel_y,ang_y,ang_z]))\n",
    "        Y.append(np.array([labels]))\n",
    "    return X, Y\n",
    "\n",
    "def clean_datset(samples, labels, fs):\n",
    "    \"\"\"\n",
    "    Clean each seample data by performing bandpass filter on range of 0.7-4\n",
    "    Hz and add FFT for each sample in the dataset\n",
    "    Args:\n",
    "        samples: list of array for each sample. each sample data has several\n",
    "         sensor measurements\n",
    "        labels: list of arrays for each sample labels\n",
    "    Returns: tuble of dataset time and frequency data for all sensors, labels\n",
    "     for each sample\n",
    "    \"\"\"\n",
    "    clean_x = []\n",
    "    clean_y = []\n",
    "    for sample, label in zip(samples, labels):\n",
    "        bandpass_filtered = butter_bandpass(sample, (0.001, 10), fs)\n",
    "        fft_sample = fft_sensors(bandpass_filtered, fs)\n",
    "        clean_x.append([bandpass_filtered,(bandpass_filtered, fft_sample)])\n",
    "        clean_y.append([int(np.mean(label)>0.5)])\n",
    "    return (clean_x, clean_y)\n",
    "def fft_sensors(data, fs):\n",
    "    \"\"\"\n",
    "    fast fourier transform for the each sensor data\n",
    "    Args:\n",
    "        data: a list of sensors measurements\n",
    "        fs: sample frequence for the signal\n",
    "    Returns:\n",
    "        return a list of arrays for all the signals in the frequency domaoin\n",
    "        data is on this format data - > sensor1_freq, sensor1_powers\n",
    "                                    - > sensor2_freq, sensor2_powers\n",
    "                                    - > sensorn_freq, sensorn_powers\n",
    "    \"\"\"\n",
    "    ff_data = []\n",
    "    # FFT each sensor measurement\n",
    "    for key in data:\n",
    "        ff_freq = np.fft.rfftfreq(len(key), 1.0 / fs)\n",
    "        ff_power = np.fft.rfft(key)\n",
    "        ff_data.append(np.array([ff_freq, ff_power]))\n",
    "    return ff_data\n",
    "\n",
    "'''data is on this format data ->sample   time_data\n",
    "                                             - > acc_x\n",
    "                                             - > acc_y\n",
    "                                             - > acc_z\n",
    "                                             - > ang_vel_x\n",
    "                                             - > ang_vel_y\n",
    "                                             - > ang_vel_z\n",
    "                                             - > ang_x\n",
    "                                             - > ang_y\n",
    "                                             - > ang_z\n",
    "                                           freq_data\n",
    "                                             - > acc_x freqs ,acc_x power \n",
    "                                             - > acc_y freqs, acc_y power\n",
    "                                             - > acc_z freqs, acc_z power\n",
    "                                             - > ang_vel_x freqs, ang_vel_x power\n",
    "                                             - > ang_vel_y freqs, ang_vel_y power\n",
    "                                             - > ang_vel_z freqs, ang_vel_z power\n",
    "                                             - > ang_x freqs, ang_x power\n",
    "                                             - > ang_y freqs, ang_y power\n",
    "                                             - > ang_z freqs, ang_z power\n",
    "'''                                                 \n",
    "\n",
    "\n",
    "def featurize_samples(samples, fs):\n",
    "    \"\"\"\n",
    "    time and freq featurization for each sample sensors data\n",
    "    Args:\n",
    "        samples: a list of the dataset samples\n",
    "        fs: sample frequency\n",
    "    Returns: return a list of arrays for each sample features\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for sample in samples:\n",
    "        # extract each sample to each sensor time & freq data\n",
    "        time_data = sample[0]\n",
    "        freqs = np.abs(sample[1][0][0])\n",
    "        freq_data=[np.abs(sensor_freq_power[1]) for sensor_freq_power in sample[1]]\n",
    "        #  average freq power for all accel axes\n",
    "        # Time features\n",
    "        min_vals = [np.min(col_data) for col_data in time_data]\n",
    "        max_vals = [np.max(col_data) for col_data in time_data]\n",
    "        mean_vals = [np.mean(col_data) for col_data in time_data]\n",
    "        median_vals=[np.median(col_data) for col_data in time_data]\n",
    "        std_vals = [np.std(col_data) for col_data in time_data]\n",
    "        var_vals = [np.var(col_data) for col_data in time_data]\n",
    "        percentile_5=[np.percentile(col_data, 5) for col_data in time_data]\n",
    "        percentile_10=[np.percentile(col_data, 10) for col_data in time_data]\n",
    "        percentile_25=[np.percentile(col_data, 25) for col_data in time_data]\n",
    "        percentile_75=[np.percentile(col_data, 75) for col_data in time_data]\n",
    "        percentile_90=[np.percentile(col_data, 90) for col_data in time_data]\n",
    "        percentile_95=[np.percentile(col_data, 95) for col_data in time_data]\n",
    "        time_features =[]\n",
    "        time_features.extend(min_vals)\n",
    "        time_features.extend(max_vals)\n",
    "        time_features.extend(median_vals)\n",
    "        time_features.extend(mean_vals)\n",
    "        time_features.extend(std_vals)\n",
    "        time_features.extend(var_vals)\n",
    "        time_features.extend(percentile_5)\n",
    "        time_features.extend(percentile_10)\n",
    "        time_features.extend(percentile_25)\n",
    "        time_features.extend(percentile_75)\n",
    "        time_features.extend(percentile_90)\n",
    "        time_features.extend(percentile_95)\n",
    "\n",
    "        total_features = time_features\n",
    "        features.append(np.array(total_features))\n",
    "    return(features)\n",
    "\n",
    "def model_train(estimators=650, depth=14, file_path=\"model_1\"):\n",
    "    \"\"\"\n",
    "    train a random forest regressor on the dataset and save it in the\n",
    "     provided path\n",
    "    Args:\n",
    "        estimators: number of trees in the model\n",
    "        depth: single value for the depth of each tree of the model\n",
    "        file_path: a path string to which model will be saved\n",
    "    Returns: return the trained regression model\n",
    "    \"\"\"\n",
    "    # Reading ref and sensors data, create timestamp for both\n",
    "    fs_imu=100\n",
    "    dir1=\"./dataset/dataset_20_08_06.csv\"\n",
    "    data_x,data_y=load_all_dataset(dir1, fs_imu, window_size=5, window_overlab=2)\n",
    "    clean_x,clean_y=clean_datset(data_x, data_y, fs_imu)\n",
    "    dataset_feats=featurize_samples(clean_x, fs_imu)\n",
    "#     train_x, test_x, train_y, test_y = train_test_split(\n",
    "#         dataset_feats, clean_y, random_state=15, test_size=0.2\n",
    "#     )\n",
    "    #print(dataset_feats.shape)\n",
    "    dataset_feats=np.array(dataset_feats)\n",
    "    \n",
    "    clean_y=np.ravel(clean_y)\n",
    "    \n",
    "    folds = StK(n_splits=5)\n",
    "    y_true=[]\n",
    "    y_pred=[]\n",
    "    for train_index, test_index in folds.split(dataset_feats, clean_y):\n",
    "        X_train, X_test = dataset_feats[train_index], dataset_feats[test_index]\n",
    "        y_train, y_test = clean_y[train_index], clean_y[test_index]\n",
    "        clf = RandomForestRegressor(\n",
    "            n_estimators=estimators, max_depth=depth, random_state=15,\n",
    "        )\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_true.extend(list(y_test))\n",
    "        y_pred.extend(clf.predict(X_test))\n",
    "    y_true=np.array(y_true)\n",
    "    y_pred=np.array(y_pred)\n",
    "    \n",
    "    with open(file_path, \"wb\") as f:\n",
    "        pickle.dump(clf, f)\n",
    "        print(\"model saved in the following dir: %s\" % file_path)\n",
    "    return clf,{\"y_true\":y_true,\"y_pred\":y_pred}\n",
    "\n",
    "def choose_best_threshold(results):\n",
    "    precision, recall, thresholds = precision_recall_curve(results[\"y_true\"],\n",
    "                                                           results[\"y_pred\"])\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    best_thresh_ind=np.argmax(F1)\n",
    "    best_thresh=thresholds[best_thresh_ind]\n",
    "    print(\"at best f-score\")\n",
    "    print(\"best thresh = \",best_thresh)\n",
    "    return best_thresh\n",
    "\n",
    "def evaluate_res(results):\n",
    "    mae_val = mean_absolute_error(results[\"y_true\"], results[\"y_pred\"])  # *60 toconvert from Hz t BPM\n",
    "    mse_val = mean_squared_error(results[\"y_true\"], results[\"y_pred\"])  # *60 to convert from Hz t BPM\n",
    "    conf_mat=confusion_matrix(results[\"y_true\"], results[\"y_pred\"],labels=[1,0])\n",
    "    \n",
    "    print(\"Model Results : \\n\")\n",
    "    print(\"mean absolute error\", mae_val)\n",
    "    print(\"mean square error\", mse_val)\n",
    "    print(\"\\nConfusion matrix\")\n",
    "    print(\"True\\Prediction\\t\\t1\\t\\t0\")\n",
    "    print(\"1\\t\\t\\t%d\\t\\t%d\"%(conf_mat[0,0],conf_mat[0,1]))\n",
    "    print(\"0\\t\\t\\t%d\\t\\t%d\"%(conf_mat[1,0],conf_mat[1,1]))\n",
    "    print(\"\\n\",classification_report(results[\"y_true\"], results[\"y_pred\"],labels=[0,1],\n",
    "                            target_names=[\"No Bump\",\"Bump\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(imu_data, model_path=\"model_1\"):\n",
    "    \"\"\"\n",
    "    load regresssion model, perform prediction over latest 5 sec of imu data\n",
    "    return prediction and confidence for each sample\n",
    "    Args:\n",
    "        data_fl: file name for sensor measurements\n",
    "        model_path: a path string to which model will be saved\n",
    "    Returns: a data for predictions and confidence of each 2s measurement\n",
    "    \"\"\"\n",
    "    fs_imu = 100\n",
    "    labels=np.zeros(len(imu_data))\n",
    "    clean_x,clean_y=clean_datset([imu_data], [labels], fs_imu)\n",
    "    dataset_feats=featurize_samples(clean_x, fs_imu)\n",
    "    dataset_feats=np.array(dataset_feats[0]).reshape(1,-1)\n",
    "    clean_y = np.ravel(clean_y)\n",
    "    reg_model = load_model(model_path)\n",
    "    samples_pred = reg_model.predict(dataset_feats)\n",
    "    \n",
    "    return (samples_pred>0.4116).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_imu = 100\n",
    "labels=np.zeros(len(imu_data))\n",
    "clean_x,clean_y=clean_datset([imu_data], [labels], fs_imu)\n",
    "dataset_feats=featurize_samples(clean_x, fs_imu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_feats[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(clean_x[0][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_x[0][0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test2=[np.random.rand(500) for i in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(test2, model_path=\"model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf,res=model_train(estimators=300, depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_im=clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_im=feat_im.reshape(12,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ind in range(6):\n",
    "    mean=feat_im[:,ind].mean()\n",
    "    maxi=feat_im[:,ind].max()\n",
    "    mini=feat_im[:,ind].min()\n",
    "    med=np.median(feat_im[:,ind])\n",
    "    print(\"max = %0.4f min = %0.4f mean = %0.4f median = %0.4f\"%(maxi,mini,mean,med) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(12):\n",
    "    mean=feat_im[ind,:].mean()\n",
    "    maxi=feat_im[ind,:].max()\n",
    "    mini=feat_im[ind,:].min()\n",
    "    med=np.median(feat_im[ind,:])\n",
    "    print(\"max = %0.4f min = %0.4f mean = %0.4f median = %0.4f\"%(maxi,mini,mean,med) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_thre=choose_best_threshold(res)\n",
    "best_thre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2=res.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2[\"y_pred\"]=(res2[\"y_pred\"]>best_thre).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2[\"y_true\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_res(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir1=\"./dataset/dataset_20_08_06.csv\"\n",
    "fs_imu=100\n",
    "data_x,data_y=load_all_dataset(dir1, fs_imu, window_size=5, window_overlab=2)\n",
    "\n",
    "clean_x,clean_y=clean_datset(data_x, data_y, fs_imu)\n",
    "\n",
    "features=featurize_samples(clean_x, fs_imu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in df.keys().values[:-1]:\n",
    "    res=df[key].values.copy()\n",
    "    res[~index_bump]=None\n",
    "    plt.figure(key)\n",
    "    plt.title(\"%s vs time\"%key)\n",
    "    plt.plot(ts,df[key].values,'b',label=key)\n",
    "    plt.plot(ts,res,'r.',label=\"bumps\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('AI_HC': conda)",
   "language": "python",
   "name": "python37664bitaihccondac6cd57a919504a4e9221a3e0dabfd326"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
